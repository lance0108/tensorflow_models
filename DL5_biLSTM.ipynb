{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a Bidirectional LSTM network to classification microblog sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lance\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from shutil import rmtree\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "seed = 123\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     5836\n",
       "Positive    2449\n",
       "Negative    1715\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(r\"C:\\Dropbox\\_projects\\word_embedding\\stocktwits\\data_final.csv\")\n",
    "df_data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ss_pos</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_overall</th>\n",
       "      <th>ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107763782</td>\n",
       "      <td>$NRG Insider \"Killinger Elizabeth R\" sold -3,9...</td>\n",
       "      <td>&lt;SYM&gt; insider \" killinger elizabeth r \" sold -...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107050038</td>\n",
       "      <td>$SGYP and in other news the Buffalo Bills ende...</td>\n",
       "      <td>&lt;SYM&gt; and in other news the buffalo bills ende...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107050388</td>\n",
       "      <td>$KMI - yep we will need the energy to heat the...</td>\n",
       "      <td>&lt;SYM&gt; - yep we will need the energy to heat th...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107055353</td>\n",
       "      <td>$XRP.X happy new year boyz, the wall at 14670 ...</td>\n",
       "      <td>&lt;SYM&gt; happy new year boyz , the wall at 0 is b...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107058260</td>\n",
       "      <td>Wabash National upgraded by ValuEngine to stro...</td>\n",
       "      <td>wabash national upgraded by valuengine to stro...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0  107763782  $NRG Insider \"Killinger Elizabeth R\" sold -3,9...   \n",
       "1  107050038  $SGYP and in other news the Buffalo Bills ende...   \n",
       "2  107050388  $KMI - yep we will need the energy to heat the...   \n",
       "3  107055353  $XRP.X happy new year boyz, the wall at 14670 ...   \n",
       "4  107058260  Wabash National upgraded by ValuEngine to stro...   \n",
       "\n",
       "                                              tokens sentiment  ss_pos  \\\n",
       "0  <SYM> insider \" killinger elizabeth r \" sold -...  Negative       1   \n",
       "1  <SYM> and in other news the buffalo bills ende...  Positive       2   \n",
       "2  <SYM> - yep we will need the energy to heat th...  Positive       1   \n",
       "3  <SYM> happy new year boyz , the wall at 0 is b...  Positive       2   \n",
       "4  wabash national upgraded by valuengine to stro...  Positive       1   \n",
       "\n",
       "   ss_neg  ss_overall        ss  \n",
       "0      -3          -2  Negative  \n",
       "1      -3          -1  Negative  \n",
       "2      -1           0   Neutral  \n",
       "3      -1           1  Positive  \n",
       "4      -1           0   Neutral  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>85</td>\n",
       "      <td>4916</td>\n",
       "      <td>4917</td>\n",
       "      <td>364</td>\n",
       "      <td>85</td>\n",
       "      <td>159</td>\n",
       "      <td>1999</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>266</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>2302</td>\n",
       "      <td>3479</td>\n",
       "      <td>1478</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1289</td>\n",
       "      <td>58</td>\n",
       "      <td>29</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>593</td>\n",
       "      <td>6</td>\n",
       "      <td>4919</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>474</td>\n",
       "      <td>72</td>\n",
       "      <td>139</td>\n",
       "      <td>4921</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4922</td>\n",
       "      <td>1010</td>\n",
       "      <td>680</td>\n",
       "      <td>59</td>\n",
       "      <td>4923</td>\n",
       "      <td>6</td>\n",
       "      <td>3481</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4    5     6     7     8     9  ...  31  32  33  \\\n",
       "0     1   104    85  4916  4917  364    85   159  1999    65 ...   0   0   0   \n",
       "1     1    18    14   266    86    5  2302  3479  1478   150 ...   0   0   0   \n",
       "2     1    21  1289    58    29  189     5   593     6  4919 ...   0   0   0   \n",
       "3     1   474    72   139  4921    7     5    91    23     3 ...   0   0   0   \n",
       "4  4922  1010   680    59  4923    6  3481     2     4     1 ...   0   0   0   \n",
       "\n",
       "   34  35  36  37  38  39  40  \n",
       "0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create padded sequences\n",
    "max_len = max(map(lambda x: len(x.split(\" \")), df_data[\"tokens\"]))\n",
    "\n",
    "t = Tokenizer(split=\" \", filters=\"\")\n",
    "t.fit_on_texts(df_data[\"tokens\"])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "sequences = t.texts_to_sequences(df_data[\"tokens\"].values)\n",
    "padded_seq = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "df_seq = pd.DataFrame(padded_seq)\n",
    "\n",
    "df_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the classes\n",
    "y_multi = to_categorical(df_data[\"sentiment\"].apply(lambda x: 1 if x == \"Positive\" else -1 if x == \"Negative\" else 0 ),\n",
    "              num_classes=3)\n",
    "y_single = np.argmax(y_multi, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "n_folds = 10\n",
    "train_size = int((n_folds - 1)/n_folds*len(df_data))\n",
    "steps = epochs*train_size//batch_size + 1\n",
    "def train_input_fn(x_train, y_train, batch_size, seed, epochs, buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"x\":x_train}, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=buffer, seed=seed, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size).prefetch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(x_test, y_test, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"x\":x_test}, y_test))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM from Cliche 2017\n",
    "def bilstm_fn(features, labels, mode, params):\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features[\"x\"], vocab_size, emb_dim, \n",
    "        initializer=params[\"embedding_initializer\"])\n",
    "    print(\"Embedding shape:\", input_layer.shape)\n",
    "    dropout1 = tf.layers.dropout(input_layer, 0.5)\n",
    "    lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(128)\n",
    "    lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(128)\n",
    "    (outputs_fw, outputs_bw), final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "        lstm_fw_cell, lstm_bw_cell, dropout1, dtype=tf.float32)\n",
    "    outputs = tf.concat([outputs_fw, outputs_bw], axis=2)\n",
    "    outputs = tf.contrib.layers.flatten(outputs)\n",
    "    dropout2 = tf.layers.dropout(outputs, 0.5)\n",
    "    print(\"Outputs shape:\", outputs.shape)\n",
    "    dense1 = tf.layers.dense(inputs=dropout2, units=30)\n",
    "    logits = tf.layers.dense(inputs=dense1, units=params[\"n_classes\"])\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        preds = {\"class\":pred_indices, \"probabilities\":probs}\n",
    "        export_outputs = {\"prediction\":tf.estimator.export.PredictOutput(preds)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=preds, export_outputs=export_outputs)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        labels_one_hot = tf.one_hot(labels, depth=params[\"n_classes\"], on_value=True, off_value=False, dtype=tf.bool)\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, pred_indices),\n",
    "            \"auroc\": tf.metrics.auc(labels_one_hot, probs)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic LSTM model\n",
    "def lstm_model_fn(features, labels, mode, params):    \n",
    "    # [batch_size x sentence_size x embedding_size]\n",
    "    inputs = tf.contrib.layers.embed_sequence(\n",
    "        features['x'], vocab_size, emb_dim,\n",
    "    initializer=params[\"embedding_initializer\"])\n",
    "\n",
    "    # create an LSTM cell of size 100\n",
    "    lstm_cell = tf.nn.rnn_cell.LSTMCell(100)\n",
    "    \n",
    "    # create the complete LSTM\n",
    "    _, final_states = tf.nn.dynamic_rnn(\n",
    "        lstm_cell, inputs, dtype=tf.float32)\n",
    "\n",
    "    # get the final hidden states of dimensionality [batch_size x sentence_size]\n",
    "    logits = tf.layers.dense(inputs=final_states.h, units=3)\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        preds = {\"class\":pred_indices, \"probabilities\":probs}\n",
    "        export_outputs = {\"prediction\":tf.estimator.export.PredictOutput(preds)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=preds, export_outputs=export_outputs)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "model_dir = r\"D:\\word_embedding\\binary\\embeddings5\"\n",
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained embeddings\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\word2vec\\GoogleNews-vectors-negative300.bin',\n",
    "#                                             binary=True)\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\glove\\test_word2vec.txt',\n",
    "#                                             binary=False)\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\fasttext\\wiki.en.vec',\n",
    "#                                             binary=False)\n",
    "\n",
    "en_model = KeyedVectors.load_word2vec_format(r'D:\\word_embedding\\balanced\\vectors1_w2v.txt',\n",
    "                                            binary=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = en_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i] = en_model.get_vector(word)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_initializer(shape=None, dtype=tf.float32, partition_info=None):\n",
    "    assert dtype is tf.float32\n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_embedding_column = tf.feature_column.embedding_column(column, \n",
    "                                                                dimension=300,\n",
    "                                                                initializer=my_initializer,\n",
    "                                                                trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277B596B9B0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1059638, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.9371\n",
      "INFO:tensorflow:loss = 0.7698147, step = 100 (4.559 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.7045\n",
      "INFO:tensorflow:loss = 0.47309375, step = 200 (4.219 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.47145337.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "1\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277BA28E780>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1032096, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.9023\n",
      "INFO:tensorflow:loss = 0.6784322, step = 100 (4.567 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.1059\n",
      "INFO:tensorflow:loss = 0.5172862, step = 200 (4.148 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.52926236.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277BCB7FE10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.075, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.5484\n",
      "INFO:tensorflow:loss = 0.58130133, step = 100 (4.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.5206\n",
      "INFO:tensorflow:loss = 0.5497128, step = 200 (4.252 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.68713194.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "3\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277BCB00400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1021576, step = 0\n",
      "INFO:tensorflow:global_step/sec: 22.199\n",
      "INFO:tensorflow:loss = 0.71059173, step = 100 (4.506 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.8455\n",
      "INFO:tensorflow:loss = 0.5516412, step = 200 (4.194 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.46318695.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "4\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277BBB5B7B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1143743, step = 0\n",
      "INFO:tensorflow:global_step/sec: 22.0968\n",
      "INFO:tensorflow:loss = 0.73476267, step = 100 (4.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.9595\n",
      "INFO:tensorflow:loss = 0.5868146, step = 200 (4.173 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.31008294.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "5\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277E58A9240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1148647, step = 0\n",
      "INFO:tensorflow:global_step/sec: 22.0725\n",
      "INFO:tensorflow:loss = 0.8934829, step = 100 (4.532 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.0463\n",
      "INFO:tensorflow:loss = 0.5140529, step = 200 (4.159 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3195327.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "6\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277E5BE55F8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.099169, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.9999\n",
      "INFO:tensorflow:loss = 0.6952628, step = 100 (4.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.0582\n",
      "INFO:tensorflow:loss = 0.41490385, step = 200 (4.157 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.46183157.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "7\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277D522B6A0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1106904, step = 0\n",
      "INFO:tensorflow:global_step/sec: 22.1655\n",
      "INFO:tensorflow:loss = 0.6658212, step = 100 (4.513 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.9483\n",
      "INFO:tensorflow:loss = 0.41214073, step = 200 (4.176 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.4927081.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "8\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277BC765E80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1052529, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.751\n",
      "INFO:tensorflow:loss = 0.6915656, step = 100 (4.598 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.9602\n",
      "INFO:tensorflow:loss = 0.39031875, step = 200 (4.174 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.39213806.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "9\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000277E5C8E470>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.111079, step = 0\n",
      "INFO:tensorflow:global_step/sec: 21.972\n",
      "INFO:tensorflow:loss = 0.7655574, step = 100 (4.552 sec)\n",
      "INFO:tensorflow:global_step/sec: 23.8118\n",
      "INFO:tensorflow:loss = 0.5695765, step = 200 (4.200 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.62062746.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Outputs shape: (?, 10496)\n",
      "Logits shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "F1: 0.647 Accuracy: 0.6624\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1 = []\n",
    "accuracy = []\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "fold = 0\n",
    "for train_idx, test_idx in skf.split(padded_seq, y_single):\n",
    "    print(fold)\n",
    "    fold+=1\n",
    "    x_train = padded_seq[train_idx]\n",
    "    x_test = padded_seq[test_idx]\n",
    "    y_train = y_single[train_idx]\n",
    "    y_test = y_single[test_idx]\n",
    "    \n",
    "    rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "    params = {'n_classes':3,\n",
    "              'embedding_initializer': my_initializer,\n",
    "#               'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0),\n",
    "              }\n",
    "    classifier = tf.estimator.Estimator(model_fn=bilstm_fn,\n",
    "                                       model_dir=model_dir,params=params)\n",
    "    classifier.train(input_fn=lambda :train_input_fn(x_train, y_train, batch_size, seed, epochs, train_size), steps=steps)\n",
    "    pred =  classifier.predict(input_fn=lambda : eval_input_fn(x_test, y_test, batch_size))\n",
    "    y_true = y_single[test_idx]\n",
    "    y_pred = [p[\"class\"] for p in pred]\n",
    "    f1.append(f1_score(y_true, y_pred, average=\"weighted\"))\n",
    "    accuracy.append(accuracy_score(y_true, y_pred))\n",
    "print(\"F1:\",round(np.mean(f1), 4), \n",
    "      \"Accuracy:\", round(np.mean(accuracy), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
