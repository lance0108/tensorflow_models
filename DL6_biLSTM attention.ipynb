{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2x BiLSTM layers with attention for classifying microblog sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network implements the LSTM model in the following paper<br>\n",
    "Baziotis, C., Pelekis, N., & Doulkeridis, C. (2017). Datastories at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017) (pp. 747-754)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lance\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from shutil import rmtree\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "seed = 123\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     5836\n",
       "Positive    2449\n",
       "Negative    1715\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(r\"C:\\Dropbox\\_projects\\word_embedding\\stocktwits\\data_final.csv\")\n",
    "df_data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>ss_pos</th>\n",
       "      <th>ss_neg</th>\n",
       "      <th>ss_overall</th>\n",
       "      <th>ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107763782</td>\n",
       "      <td>$NRG Insider \"Killinger Elizabeth R\" sold -3,9...</td>\n",
       "      <td>&lt;SYM&gt; insider \" killinger elizabeth r \" sold -...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>-2</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>107050038</td>\n",
       "      <td>$SGYP and in other news the Buffalo Bills ende...</td>\n",
       "      <td>&lt;SYM&gt; and in other news the buffalo bills ende...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>107050388</td>\n",
       "      <td>$KMI - yep we will need the energy to heat the...</td>\n",
       "      <td>&lt;SYM&gt; - yep we will need the energy to heat th...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107055353</td>\n",
       "      <td>$XRP.X happy new year boyz, the wall at 14670 ...</td>\n",
       "      <td>&lt;SYM&gt; happy new year boyz , the wall at 0 is b...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>107058260</td>\n",
       "      <td>Wabash National upgraded by ValuEngine to stro...</td>\n",
       "      <td>wabash national upgraded by valuengine to stro...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0  107763782  $NRG Insider \"Killinger Elizabeth R\" sold -3,9...   \n",
       "1  107050038  $SGYP and in other news the Buffalo Bills ende...   \n",
       "2  107050388  $KMI - yep we will need the energy to heat the...   \n",
       "3  107055353  $XRP.X happy new year boyz, the wall at 14670 ...   \n",
       "4  107058260  Wabash National upgraded by ValuEngine to stro...   \n",
       "\n",
       "                                              tokens sentiment  ss_pos  \\\n",
       "0  <SYM> insider \" killinger elizabeth r \" sold -...  Negative       1   \n",
       "1  <SYM> and in other news the buffalo bills ende...  Positive       2   \n",
       "2  <SYM> - yep we will need the energy to heat th...  Positive       1   \n",
       "3  <SYM> happy new year boyz , the wall at 0 is b...  Positive       2   \n",
       "4  wabash national upgraded by valuengine to stro...  Positive       1   \n",
       "\n",
       "   ss_neg  ss_overall        ss  \n",
       "0      -3          -2  Negative  \n",
       "1      -3          -1  Negative  \n",
       "2      -1           0   Neutral  \n",
       "3      -1           1  Positive  \n",
       "4      -1           0   Neutral  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>85</td>\n",
       "      <td>4916</td>\n",
       "      <td>4917</td>\n",
       "      <td>364</td>\n",
       "      <td>85</td>\n",
       "      <td>159</td>\n",
       "      <td>1999</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>266</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>2302</td>\n",
       "      <td>3479</td>\n",
       "      <td>1478</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1289</td>\n",
       "      <td>58</td>\n",
       "      <td>29</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>593</td>\n",
       "      <td>6</td>\n",
       "      <td>4919</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>474</td>\n",
       "      <td>72</td>\n",
       "      <td>139</td>\n",
       "      <td>4921</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4922</td>\n",
       "      <td>1010</td>\n",
       "      <td>680</td>\n",
       "      <td>59</td>\n",
       "      <td>4923</td>\n",
       "      <td>6</td>\n",
       "      <td>3481</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4    5     6     7     8     9  ...  31  32  33  \\\n",
       "0     1   104    85  4916  4917  364    85   159  1999    65 ...   0   0   0   \n",
       "1     1    18    14   266    86    5  2302  3479  1478   150 ...   0   0   0   \n",
       "2     1    21  1289    58    29  189     5   593     6  4919 ...   0   0   0   \n",
       "3     1   474    72   139  4921    7     5    91    23     3 ...   0   0   0   \n",
       "4  4922  1010   680    59  4923    6  3481     2     4     1 ...   0   0   0   \n",
       "\n",
       "   34  35  36  37  38  39  40  \n",
       "0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create padded sequences\n",
    "max_len = max(map(lambda x: len(x.split(\" \")), df_data[\"tokens\"]))\n",
    "\n",
    "t = Tokenizer(split=\" \", filters=\"\")\n",
    "t.fit_on_texts(df_data[\"tokens\"])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "sequences = t.texts_to_sequences(df_data[\"tokens\"].values)\n",
    "padded_seq = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "df_seq = pd.DataFrame(padded_seq)\n",
    "\n",
    "df_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the classes\n",
    "y_multi = to_categorical(df_data[\"sentiment\"].apply(lambda x: 1 if x == \"Positive\" else -1 if x == \"Negative\" else 0 ),\n",
    "              num_classes=3)\n",
    "y_single = np.argmax(y_multi, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 2\n",
    "n_folds = 10\n",
    "train_size = int((n_folds - 1)/n_folds*len(df_data))\n",
    "steps = epochs*train_size//batch_size + 1\n",
    "def train_input_fn(x_train, y_train, batch_size, seed, epochs, buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"x\":x_train}, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=buffer, seed=seed, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size).prefetch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(x_test, y_test, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"x\":x_test}, y_test))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTM from Baziotis et al 2017\n",
    "def bilstm_fn(features, labels, mode, params):\n",
    "    attn_size = 1\n",
    "    n_units = 150\n",
    "    cell_dropout = 0.25\n",
    "    # embedding\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features[\"x\"], vocab_size, emb_dim, trainable=False,\n",
    "        initializer=params[\"embedding_initializer\"])\n",
    "    print(\"Embedding shape:\", input_layer.shape)\n",
    "    \n",
    "    # Gaussian noise\n",
    "    noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=0.2, dtype=tf.float32)\n",
    "    print(\"Noise shape:\", noise.shape)\n",
    "    input_layer = tf.add(input_layer, noise)\n",
    "    print(\"Noise added shape:\", input_layer.shape)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(input_layer, 0.3)\n",
    "    print(\"Dropout1 shape:\", dropout1.shape)\n",
    "    \n",
    "    # BiLSTM 1\n",
    "    with tf.variable_scope(\"BiLSTM1\", reuse=tf.AUTO_REUSE):\n",
    "        lstm_fw_cell1 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_units), cell_dropout)\n",
    "        lstm_bw_cell1 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_units), cell_dropout)\n",
    "        (outputs_fw1, outputs_bw1), final_states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            lstm_fw_cell1, lstm_bw_cell1, dropout1, dtype=tf.float32)\n",
    "        outputs1 = tf.concat([outputs_fw1, outputs_bw1], axis=2)\n",
    "        print(\"Outputs1 shape:\", outputs1.shape)\n",
    "    dropout2 = tf.layers.dropout(outputs1, 0.5)\n",
    "    # BiLSTM 2\n",
    "    with tf.variable_scope(\"BiLSTM2\", reuse=tf.AUTO_REUSE):\n",
    "        lstm_fw_cell2 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_units), cell_dropout)\n",
    "        lstm_bw_cell2 = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(n_units), cell_dropout)\n",
    "        (outputs_fw2, outputs_bw2), final_states2 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            lstm_fw_cell2, lstm_bw_cell2, dropout2, dtype=tf.float32)\n",
    "        outputs2 = tf.concat([outputs_fw2, outputs_bw2], axis=2)\n",
    "        print(\"Outputs2 shape:\", outputs2.shape)\n",
    "        \n",
    "    dropout3 = tf.layers.dropout(outputs2, 0.5)\n",
    "        \n",
    "    # Attention\n",
    "    with tf.variable_scope(\"Attention\", reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable(\"W\", [n_units*2, attn_size])\n",
    "        b = tf.get_variable(\"b\", [attn_size])\n",
    "        e = tf.tanh(tf.tensordot(dropout3, W, axes=1) + b)\n",
    "        a = tf.nn.softmax(e)\n",
    "        print(\"a shape:\", a.shape)\n",
    "        r = tf.multiply(a, dropout3)\n",
    "        print(\"r shape:\", r.shape)\n",
    "        \n",
    "    r = tf.contrib.layers.flatten(r)\n",
    "    logits = tf.layers.dense(inputs=r, units=params[\"n_classes\"])\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    print([v.name for v in tf.trainable_variables()])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        preds = {\"class\":pred_indices, \"probabilities\":probs}\n",
    "        export_outputs = {\"prediction\":tf.estimator.export.PredictOutput(preds)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=preds, export_outputs=export_outputs)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    l2_loss = tf.reduce_sum([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \"bias\" not in v.name ])\n",
    "    loss = loss + 0.0001*l2_loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "#         train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        # Gradient clipping by norm\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs]\n",
    "        train_op = optimizer.apply_gradients(capped_gvs, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         probs = tf.nn.softmax(logits)\n",
    "#         pred_indices = tf.argmax(probs, 1)\n",
    "#         labels_one_hot = tf.one_hot(labels, depth=params[\"n_classes\"], on_value=True, off_value=False, dtype=tf.bool)\n",
    "#         eval_metric_ops = {\n",
    "#             \"accuracy\": tf.metrics.accuracy(labels, pred_indices),\n",
    "#             \"auroc\": tf.metrics.auc(labels_one_hot, probs)\n",
    "#         }\n",
    "#         return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom model\n",
    "def bilstm_fn2(features, labels, mode, params):\n",
    "    attn_size = 1\n",
    "    n_units = 150\n",
    "    # embedding\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features[\"x\"], vocab_size, emb_dim, trainable=False,\n",
    "        initializer=params[\"embedding_initializer\"])\n",
    "    print(\"Embedding shape:\", input_layer.shape)\n",
    "    \n",
    "    # Gaussian noise (this seems to cause performance decrease)\n",
    "#     noise = tf.random_normal(shape=tf.shape(input_layer), mean=0.0, stddev=0.2, dtype=tf.float32)\n",
    "#     print(\"Noise shape:\", noise.shape)\n",
    "#     input_layer = tf.add(input_layer, noise)\n",
    "#     print(\"Noise added shape:\", input_layer.shape)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(input_layer, 0.3)\n",
    "    print(\"Dropout1 shape:\", dropout1.shape)\n",
    "    \n",
    "    # BiLSTM 1\n",
    "    with tf.variable_scope(\"BiLSTM1\", reuse=tf.AUTO_REUSE):\n",
    "        lstm_fw_cell1 = tf.nn.rnn_cell.LSTMCell(n_units)\n",
    "        lstm_bw_cell1 = tf.nn.rnn_cell.LSTMCell(n_units)\n",
    "        (outputs_fw1, outputs_bw1), final_states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            lstm_fw_cell1, lstm_bw_cell1, dropout1, dtype=tf.float32)\n",
    "        outputs1 = tf.concat([outputs_fw1, outputs_bw1], axis=2)\n",
    "        print(\"Outputs1 shape:\", outputs1.shape)\n",
    "    # BiLSTM 2\n",
    "    with tf.variable_scope(\"BiLSTM2\", reuse=tf.AUTO_REUSE):\n",
    "        lstm_fw_cell2 = tf.nn.rnn_cell.LSTMCell(n_units)\n",
    "        lstm_bw_cell2 = tf.nn.rnn_cell.LSTMCell(n_units)\n",
    "        (outputs_fw2, outputs_bw2), final_states2 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            lstm_fw_cell2, lstm_bw_cell2, outputs1, dtype=tf.float32)\n",
    "        outputs2 = tf.concat([outputs_fw2, outputs_bw2], axis=2)\n",
    "        print(\"Outputs2 shape:\", outputs2.shape)\n",
    "        \n",
    "    # Attention\n",
    "    with tf.variable_scope(\"Attention\", reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable(\"W\", [n_units*2, attn_size])\n",
    "        b = tf.get_variable(\"b\", [attn_size])\n",
    "        e = tf.tanh(tf.tensordot(outputs2, W, axes=1) + b)\n",
    "        a = tf.nn.softmax(e)\n",
    "        print(\"a shape:\", a.shape)\n",
    "        r = tf.multiply(a, outputs2)\n",
    "        print(\"r shape:\", r.shape)\n",
    "        \n",
    "    r = tf.contrib.layers.flatten(r)\n",
    "    logits = tf.layers.dense(inputs=r, units=params[\"n_classes\"])\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "    print([v.name for v in tf.trainable_variables()])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        preds = {\"class\":pred_indices, \"probabilities\":probs}\n",
    "        export_outputs = {\"prediction\":tf.estimator.export.PredictOutput(preds)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=preds, export_outputs=export_outputs)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "    l2_loss = tf.reduce_sum([tf.nn.l2_loss(v) for v in tf.trainable_variables() if \"bias\" not in v.name ])\n",
    "    loss = loss + 0.0001*l2_loss\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "#         train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        # Gradient clipping by norm\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5), var) for grad, var in gvs]\n",
    "        train_op = optimizer.apply_gradients(capped_gvs, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         probs = tf.nn.softmax(logits)\n",
    "#         pred_indices = tf.argmax(probs, 1)\n",
    "#         labels_one_hot = tf.one_hot(labels, depth=params[\"n_classes\"], on_value=True, off_value=False, dtype=tf.bool)\n",
    "#         eval_metric_ops = {\n",
    "#             \"accuracy\": tf.metrics.accuracy(labels, pred_indices),\n",
    "#             \"auroc\": tf.metrics.auc(labels_one_hot, probs)\n",
    "#         }\n",
    "#         return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "model_dir = r\"D:\\word_embedding\\binary\\embeddings5\"\n",
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained embeddings\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\word2vec\\GoogleNews-vectors-negative300.bin',\n",
    "#                                             binary=True)\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\glove\\test_word2vec.txt',\n",
    "#                                             binary=False)\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\fasttext\\wiki.en.vec',\n",
    "#                                             binary=False)\n",
    "\n",
    "en_model = KeyedVectors.load_word2vec_format(r'D:\\word_embedding\\balanced\\vectors1_w2v.txt',\n",
    "                                            binary=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = en_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i] = en_model.get_vector(word)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_initializer(shape=None, dtype=tf.float32, partition_info=None):\n",
    "    assert dtype is tf.float32\n",
    "    return embedding_matrix\n",
    "\n",
    "# pretrained_embedding_column = tf.feature_column.embedding_column(column, \n",
    "#                                                                 dimension=300,\n",
    "#                                                                 initializer=my_initializer,\n",
    "#                                                                 trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEED9F7940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.195881, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.95989\n",
      "INFO:tensorflow:loss = 0.76894146, step = 100 (10.041 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.71200323.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "1\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEB03A6710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2001387, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.21082\n",
      "INFO:tensorflow:loss = 0.8350193, step = 100 (10.858 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.74597514.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEABF16860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1964679, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.50575\n",
      "INFO:tensorflow:loss = 0.7904998, step = 100 (10.521 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7513771.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "3\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEAE996A58>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2045195, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.3227\n",
      "INFO:tensorflow:loss = 0.8272812, step = 100 (10.727 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6553514.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "4\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEABF3F630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2011305, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.22572\n",
      "INFO:tensorflow:loss = 0.82968, step = 100 (10.841 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.61148435.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "5\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEAF286940>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2068336, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.23023\n",
      "INFO:tensorflow:loss = 0.79629385, step = 100 (10.835 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.71077543.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "6\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEAFA97F28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.193386, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.01272\n",
      "INFO:tensorflow:loss = 0.6950873, step = 100 (11.096 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.8332039.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "7\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEABF2C860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1981568, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.05842\n",
      "INFO:tensorflow:loss = 0.690775, step = 100 (11.041 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.8315222.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "8\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEABBDBEB8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1981558, step = 0\n",
      "INFO:tensorflow:global_step/sec: 8.87046\n",
      "INFO:tensorflow:loss = 0.6759421, step = 100 (11.274 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.80931896.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "9\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings5', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001AEB2931860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1917709, step = 0\n",
      "INFO:tensorflow:global_step/sec: 9.13207\n",
      "INFO:tensorflow:loss = 0.72613376, step = 100 (10.950 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 141 into D:\\word_embedding\\binary\\embeddings5\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.7177913.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Embedding shape: (?, 41, 300)\n",
      "Dropout1 shape: (?, 41, 300)\n",
      "Outputs1 shape: (?, 41, 300)\n",
      "Outputs2 shape: (?, 41, 300)\n",
      "a shape: (?, 41, 1)\n",
      "r shape: (?, 41, 300)\n",
      "Logits shape: (?, 3)\n",
      "['BiLSTM1/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM1/bidirectional_rnn/bw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/fw/lstm_cell/bias:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/kernel:0', 'BiLSTM2/bidirectional_rnn/bw/lstm_cell/bias:0', 'Attention/W:0', 'Attention/b:0', 'dense/kernel:0', 'dense/bias:0']\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings5\\model.ckpt-141\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "F1: 0.6366 Accuracy: 0.6562\n",
      "Wall time: 4min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f1 = []\n",
    "accuracy = []\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "fold = 0\n",
    "for train_idx, test_idx in skf.split(padded_seq, y_single):\n",
    "    print(fold)\n",
    "    fold+=1\n",
    "    x_train = padded_seq[train_idx]\n",
    "    x_test = padded_seq[test_idx]\n",
    "    y_train = y_multi[train_idx]\n",
    "    y_test = y_multi[test_idx]\n",
    "    \n",
    "    rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "    params = {'n_classes':3,\n",
    "              'embedding_initializer': my_initializer,\n",
    "#               'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0),\n",
    "              }\n",
    "    classifier = tf.estimator.Estimator(model_fn=bilstm_fn2,\n",
    "                                       model_dir=model_dir,params=params)\n",
    "    classifier.train(input_fn=lambda :train_input_fn(x_train, y_train, batch_size, seed, epochs, train_size), steps=steps)\n",
    "    pred =  classifier.predict(input_fn=lambda : eval_input_fn(x_test, y_test, batch_size))\n",
    "    y_true = y_single[test_idx]\n",
    "    y_pred = [p[\"class\"] for p in pred]\n",
    "    f1.append(f1_score(y_true, y_pred, average=\"weighted\"))\n",
    "    accuracy.append(accuracy_score(y_true, y_pred))\n",
    "print(\"F1:\",round(np.mean(f1), 4), \n",
    "      \"Accuracy:\", round(np.mean(accuracy), 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
