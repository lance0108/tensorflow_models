{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: 1) initialize embeddings with pretrained. 2) shuffle inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Restore model from checkpoint.<br>\n",
    "2. Evaluate word embeddings using closest words.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy model definition and classifier configuration from the training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'F:\\\\w2v_model_dir_8192', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1200, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x0000016852B48470>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016852B488D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n"
     ]
    }
   ],
   "source": [
    "def sg_model_fn(features, labels, mode):\n",
    "#     print(\"X shape:\", features[\"word\"].shape)\n",
    "#     print(\"Y shape:\", labels.shape)\n",
    "\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embeddings = tf.get_variable(\"embedding\", shape=[vocab_size, emb_dim])\n",
    "        embed = tf.nn.embedding_lookup(embeddings, features[\"word\"])\n",
    "        print(\"Embedded shape:\", embed.shape)\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        nce_weights = tf.get_variable(\"W\", shape=[vocab_size, emb_dim])\n",
    "        print(\"nce_weights shape:\", nce_weights.shape)\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        nce_biases = tf.get_variable(\"b\", shape=[vocab_size])\n",
    "        print(\"nce_biases shape:\", nce_biases.shape)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "            weights=nce_weights, biases=nce_biases,\n",
    "            inputs=embed, labels=labels[:, None], \n",
    "            num_sampled=5, num_classes=vocab_size))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"global_step\", tf.train.get_global_step())\n",
    "#     tf.summary.text(\"embeddings\", tf.constant(np.array([tf.print(embeddings)])))\n",
    "    tf.summary.text(\"my_summary\", tf.constant(np.array([\"Hello\", \"Tensorflow!\"])))\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\w2v_model_dir_8192\\model.ckpt-351405\n",
      "351405\n",
      "['W', 'W/Adam', 'W/Adam_1', 'b', 'b/Adam', 'b/Adam_1', 'beta1_power', 'beta2_power', 'embedding', 'embedding/Adam', 'embedding/Adam_1', 'global_step']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3489467\n",
      "3489467\n"
     ]
    }
   ],
   "source": [
    "# read word indexes\n",
    "index2word = {}\n",
    "word2index = {}\n",
    "with open(r\"D:\\stocktwits_text\\2018 all\\word_index.csv\", \"r\", encoding=\"utf-8\") as in_f:\n",
    "    for row in csv.reader(in_f):\n",
    "        index2word[int(row[1])] = row[0]\n",
    "        word2index[row[0]] = int(row[1])\n",
    "print(len(index2word))\n",
    "print(len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained embeddings\n",
    "def evaluate(model_dir):\n",
    "    strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2, prefetch_on_device=True)\n",
    "    config = tf.estimator.RunConfig(\n",
    "        save_summary_steps=200,\n",
    "        train_distribute=strategy,\n",
    "        save_checkpoints_secs = 20*60,\n",
    "        keep_checkpoint_max = 3,\n",
    "        model_dir=model_dir)\n",
    "    classifier = tf.estimator.Estimator(model_fn=sg_model_fn, \n",
    "                                        config=config)\n",
    "    print(classifier.latest_checkpoint())\n",
    "    print(classifier.get_variable_value(\"global_step\"))\n",
    "    print(classifier.get_variable_names())\n",
    "    \n",
    "    fixed_words = [\"bull\", \"bullish\", \"bear\", \"bearish\", \"breakout\", \"bottom\", \"bounce\", \"short\", \"rally\", \"strength\",\n",
    "                   \"nice\", \"dip\",\"puts\",\"aapl\",\"down\",\"earnings\", \"dividend\", \"subpoena\", \"estimate\", \"volume\", \"risk\",\n",
    "                   \":)\", \"<rocket>\", \":(\", \"<fire>\"]\n",
    "    fixed_examples = [word2index[w] for w in fixed_words]\n",
    "    valid_size = 16               # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "\n",
    "    embeddings = classifier.get_variable_value(\"embedding\")\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, fixed_examples)\n",
    "    norm_embed = normalize(embeddings)\n",
    "    valid_embed = np.vstack(list(map(lambda x: norm_embed[x], valid_examples)))\n",
    "    similarity = np.matmul(valid_embed, np.transpose(norm_embed))\n",
    "\n",
    "    for i in range(len(valid_examples)):\n",
    "        valid_word = index2word[valid_examples[i]]\n",
    "        top_k = 8 # number of nearest neighbors\n",
    "        nearest = (-similarity[i, :]).argsort()[1:top_k+1]\n",
    "        log = '%12s:\\t' % valid_word\n",
    "        for k in range(top_k):\n",
    "            close_word = index2word[nearest[k]]\n",
    "            log = '%s %s,' % (log, close_word)\n",
    "        print(log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'F:\\\\w2v_model_dir', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1200, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x0000016852B3BC50>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016852B3BB70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "F:\\w2v_model_dir\\model.ckpt-87852\n",
      "87852\n",
      "['W', 'W/Adam', 'W/Adam_1', 'b', 'b/Adam', 'b/Adam_1', 'beta1_power', 'beta2_power', 'embedding', 'embedding/Adam', 'embedding/Adam_1', 'global_step']\n",
      "          up:\t down, uo, upp, fown, tmro, anther, dwon, dowm,\n",
      "        time:\t tine, tome, tme, weeknd, 0ies, june-july, highes, 0mons,\n",
      "          we:\t haps, tmorrow, ya'll, we'll, 0.we, tomororw, tomorrw, soonish,\n",
      "          on:\t with, <sym>, rht, bbby, and, 0sep, earring, 0oct,\n",
      "           (:\t ), rydex, cls, lon, esh, :op, nysearca, #swing,\n",
      "          to:\t and, eill, tomororw, iwll, ot, wiill, throu, tmro,\n",
      "       would:\t wouldn't, wouldnt, it'd, wud, could, wld, you'd, youd,\n",
      "           ,:\t ., wlt, anf, vips, jks, fizz, bita, oclr,\n",
      "       idiot:\t asshole, imbecile, amateur, moron, ahole, asshat, a-hole, dumbass,\n",
      "       leave:\t save, bury, eat, haunt, vaporize, spoil, marinate, baghold,\n",
      "       raise:\t raising, generate, reduce, refinance, incur, convert, repay, buyback,\n",
      "       offer:\t authorize, shelf, secondary, swap, anounce, aquire, offering, aspire,\n",
      "      rather:\t better, safer, larger, bigger, greater, quicker, riskier, smaller,\n",
      "         0nd:\t 0rd, 0st, final, november, october, september, august, december,\n",
      " <thumbs_up>:\t <thumbs_up_light_skin_tone>, <ok_hand>, <thumbs_up_medium-light_skin_tone>, <smiling_face_with_sunglasses>, <thumbs_up_medium_skin_tone>, <smiling_face_with_smiling_eyes>, <folded_hands>, <clinking_beer_mugs>,\n",
      "     neutral:\t overweight, underweight, nomura, argus, outperform, cibc, keybanc, equal-weight,\n",
      "        bull:\t bear, homing, textbook, engulf, trap, saucer, penant, hns,\n",
      "     bullish:\t bearish, leaning, overbought, homing, constructive, crossovers, indecisive, heikin,\n",
      "        bear:\t bull, shark, trap, homing, hollow, pigeon, crab, flaggy,\n",
      "     bearish:\t bullish, crossovers, overbought, candlesticks, constructive, oscillator, homing, renko,\n",
      "    breakout:\t reversal, continuation, breakdown, piercing, pennant, kumo, bullflag, flagging,\n",
      "      bottom:\t btm, decending, prz, backtest, exhaustion, rejection, 0wma, floor,\n",
      "      bounce:\t re-test, rebound, pull-back, deadcat, pullback, retrace, test, retest,\n",
      "       short:\t shrt, sml, daytrade, positional, ★, trm, posn, uco,\n",
      "       rally:\t rebound, sell-off, selloff, recovery, short-covering, whipsaw, snapback, swoon,\n",
      "    strength:\t weakness, indecision, momentum, breadth, divergent, reversals, longer-term, breakouts,\n",
      "        nice:\t lovely, sweet, tasty, juicy, fantastic, great, beautiful, bouncer,\n",
      "         dip:\t dipper, afternoon, gap-up, lunchtime, morn, re-load, pop, leap,\n",
      "        puts:\t calls, itm, weeklies, otm, leaps, ditm, 0calls, lottos,\n",
      "        aapl:\t amzn, nflx, twtr, baba, nvda, tsla, mu, celg,\n",
      "        down:\t up, uo, donw, kospi, fown, dowm, to0, dwn,\n",
      "    earnings:\t earning, thursday, wednesday, er, tuesday, after, ex-div, 0q,\n",
      "    dividend:\t aristocrat, putnam, semiannual, reinvestment, series, nuveen, buy-write, costamare,\n",
      "    subpoena:\t whistleblower, foia, prosecutor, investigator, investigators, commissioner, subpoenas, investigative,\n",
      "    estimate:\t projection, forcast, exceeded, estimation, concensus, dau, subscriber, narrowed,\n",
      "      volume:\t unusually, volum, cumulative, volumes, mvg, short, volumn, #volume,\n",
      "        risk:\t tolerance, reward, favors, outweighs, appetite, aversion, averse, liquidity,\n",
      "          :):\t :-), ;), =), ;-), (:, :d, :], :-d,\n",
      "    <rocket>:\t <collision>, <fire>, <chart_increasing>, <dollar_banknote>, <money_bag>, <volcano>, <flexed_biceps_light_skin_tone>, <flexed_biceps_medium-light_skin_tone>,\n",
      "          :(:\t :-(, ;(, =(, :'(, ugh, :-/, ;-(, grrr,\n",
      "      <fire>:\t <collision>, <rocket>, <heavy_dollar_sign>, <chart_increasing>, <dollar_banknote>, <money_bag>, <volcano>, <water_buffalo>,\n"
     ]
    }
   ],
   "source": [
    "evaluate(r\"F:\\w2v_model_dir\") # second epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'F:\\\\w2v_model_dir_8192', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1200, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x0000016852B3B668>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016852B3BC88>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "F:\\w2v_model_dir_8192\\model.ckpt-702810\n",
      "702810\n",
      "['W', 'W/Adam', 'W/Adam_1', 'b', 'b/Adam', 'b/Adam_1', 'beta1_power', 'beta2_power', 'embedding', 'embedding/Adam', 'embedding/Adam_1', 'global_step']\n",
      "           /:\t o, o's, w, kilometres, utiful, m-cap, ufe, /o=,\n",
      "       short:\t shrt, shorter, shorts, shorting, farkers, pre-lim, shorty, #morningstar,\n",
      "           a:\t warby, another, tedium, invol, thuggery, distal, droppy, 0success,\n",
      "          up:\t down, uo, spitball, preowned, down.lol, chevy's, nourishing, anaerobic,\n",
      "         the:\t pvam, damodaran's, tje, repairable, stting, li's, #bombardier, graincorp,\n",
      "          ..:\t ..., them.the, nmo, froyo, dorong, complaceny, zoomlion, y0day,\n",
      "        long:\t longggg, ling, lomg, longish, loooong, looooooong, med-long, loooooong,\n",
      "      market:\t mkt, mrkt, markets, maket, markt, marlet, mkts, mrket,\n",
      "     heading:\t headed, going, marching, headin, cruising, gong, goin, goong,\n",
      "      aren't:\t arent, weren't, arnt, are, aren, they're, werent, theyre,\n",
      "         0nd:\t 0rd, 0st, second, 0th, first, 0rth, unblinding, third,\n",
      "    disposed:\t cashed-in, declared, sr, ofcr, owning, svp, counsel, controller,\n",
      "        moon:\t moooon, <crescent_moon>, pluto, mars, mooooon, moooooooon, mooon, moooooon,\n",
      "      moment:\t movemen, movem, moveme, cavendish, dentist's, farnborough, alopecia, #snrtg,\n",
      "    industry:\t miscellaneous, avera, averag, nonstore, indus, indust, roa, outperforms,\n",
      "     chinese:\t china, japanese, israeli, <china>, tech, weed, indian, chines,\n",
      "        bull:\t bear, raging, flag, perma, secular, trap, traps, falg,\n",
      "     bullish:\t bearish, bulish, bearis, optimistic, bullis, beari, bullush, englufing,\n",
      "        bear:\t bull, raging, perma, trap, teddy, traps, trappy, pooh,\n",
      "     bearish:\t bullish, bulish, bearis, negative, beari, bullis, bullush, engulfings,\n",
      "    breakout:\t break-out, breakdown, reversal, cup-with-handle, bullflag, pennant, vcp, bull-flag,\n",
      "      bottom:\t top, bottoms, btm, floor, entendre, botton, bottome, buttom,\n",
      "      bounce:\t bounces, rebound, retrace, bouce, cat, dcb, deadcat, bouncing,\n",
      "       short:\t shrt, shorter, shorts, shorting, farkers, pre-lim, shorty, #morningstar,\n",
      "       rally:\t selloff, sell-off, rebound, meltdown, snap-back, short-covering, resurge, swoon,\n",
      "    strength:\t weakness, strenght, resilience, strengh, weekness, resiliency, relative, stength,\n",
      "        nice:\t lovely, awesome, sweet, beautiful, fantastic, great, fabulous, decent,\n",
      "         dip:\t dips, dipppp, deeps, dippage, spike, dipper, kippur, pulllback,\n",
      "        puts:\t calls, weeklies, leaps, itm, lottos, otm, ditm, 0calls,\n",
      "        aapl:\t nflx, amzn, appl, msft, googl, goog, baba, nvda,\n",
      "        down:\t up, dwn, diwn, dowm, uo, doen, down.lol, re-weighting,\n",
      "    earnings:\t earning, er, earnigs, earrings, earings, computex, earning's, earnigns,\n",
      "    dividend:\t dividends, divy, divi, div, divvy, dividen, payout, yield,\n",
      "    subpoena:\t subpoenas, whistleblower, doj, inquiry, rescind, allegation, alleging, appellate,\n",
      "    estimate:\t projection, estimation, projections, expectation, estimated, projected, guesstimate, forcast,\n",
      "      volume:\t volumes, volumn, vol, voume, volum, volumne, colume, vollume,\n",
      "        risk:\t reward, risks, gamble, tolerance, asymmetric, diversification, tradeoff, risking,\n",
      "          :):\t :-), ;), ;-), =), <winking_face>, <smiling_face_with_smiling_eyes>, <slightly_smiling_face>, <beaming_face_with_smiling_eyes>,\n",
      "    <rocket>:\t <collision>, <chart_increasing>, <airplane_departure>, <money_with_wings>, <heavy_dollar_sign>, <money-mouth_face>, <dollar_banknote>, <fire>,\n",
      "          :(:\t :-(, ugh, :/, ;(, <pensive_face>, :'(, <disappointed_face>, fml,\n",
      "      <fire>:\t <collision>, <heavy_dollar_sign>, <rocket>, <chart_increasing>, <money_with_wings>, <bomb>, <money-mouth_face>, <comet>,\n"
     ]
    }
   ],
   "source": [
    "evaluate(r\"F:\\w2v_model_dir_8192\") #8192 seems to be much better than 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing RunConfig with distribution strategies.\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'F:\\\\w2v_model_dir_8192_shuffle', '_tf_random_seed': None, '_save_summary_steps': 200, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 1200, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 3, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x0000016852B3BCC0>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000016852B3BA90>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n",
      "F:\\w2v_model_dir_8192_shuffle\\model.ckpt-351405\n",
      "351405\n",
      "['W', 'W/Adam', 'W/Adam_1', 'b', 'b/Adam', 'b/Adam_1', 'beta1_power', 'beta2_power', 'embedding', 'embedding/Adam', 'embedding/Adam_1', 'global_step']\n",
      "           ,:\t ., ..., hansberger, szp, belsmoke, mbtn, rakhain, fincas,\n",
      "        like:\t liek, lik, lile, ike, llike, loke, lije, lke,\n",
      "          in:\t inthe, jn, moviegoing, prsentation, 0qts, capac, relistening, no-debt,\n",
      "        down:\t up, dwn, uo, dowm, doen, diwn, dn, to0,\n",
      "          at:\t @, at0, 0at, stupidly, satoshis, atthe, ii's, arounf,\n",
      "         i'm:\t im, am, i'am, iam, you're, youre, we're, your'e,\n",
      "         lol:\t lmao, lolol, hahaha, hahah, haha, hahahah, lolz, xd,\n",
      "         and:\t snd, abd, nd, ans, janq, aprq, octq, stops.you,\n",
      "         0nd:\t 0rd, 0st, second, first, 0th, 0rst, ninth, third,\n",
      "      you'll:\t youll, u'll, ull, they'll, we'll, he'll, i'll, theyll,\n",
      "      aren't:\t arent, arnt, weren't, are, aren, werent, arn't, they're,\n",
      "           £:\t €, ¥, rupees, ₩, euros, bn, 0bn, ¦,\n",
      "       phase:\t clinical, ph0, trial, trials, p0b, preclinical, cohort, pre-clinical,\n",
      "    comments:\t posts, rants, arguments, comment, tweets, postings, replies, messages,\n",
      "     #stocks:\t #banks, #quant, #business, #sp0, #markets, #tr, #retail, #stock,\n",
      "      nobody:\t noone, no-one, no0, everybody, everyone, nobody's, eveyone, everone,\n",
      "        bull:\t bear, secular, raging, perma, permabull, camp, trap, traps,\n",
      "     bullish:\t bearish, bulish, bulllish, hopeful, bullis, bullsih, skeptical, boolish,\n",
      "        bear:\t bull, raging, perma, secular, panda, permabull, trap, grizzly,\n",
      "     bearish:\t bullish, bulish, negative, permabear, bearis, berish, bullis, bearishness,\n",
      "    breakout:\t break-out, reversal, thrust, breakdown, bullflag, coil, explosion, triangle,\n",
      "      bottom:\t btm, floor, bottoms, top, buttom, dbl, botton, bottem,\n",
      "      bounce:\t bouce, rebound, retrace, bounces, deadcat, dcb, cat, pull,\n",
      "       short:\t ahort, shrt, shorter, short-medium, shor, iborrowdesk, nuven, lomg,\n",
      "       rally:\t sell-off, selloff, meltdown, downturn, swoon, rebound, retreat, plunge,\n",
      "    strength:\t weakness, strenght, resilience, strengh, resiliency, weekness, weakening, hesitation,\n",
      "        nice:\t lovely, sweet, beautiful, neat, decent, awesome, fantastic, great,\n",
      "         dip:\t pullback, dips, oppurtunity, pull-back, dipper, spike, dipp, oportunity,\n",
      "        puts:\t calls, weeklies, leaps, 0calls, otm, lottos, itm, 0puts,\n",
      "        aapl:\t amzn, nflx, baba, msft, appl, googl, nvda, tsla,\n",
      "        down:\t up, dwn, uo, dowm, doen, diwn, dn, to0,\n",
      "    earnings:\t earning, er, earrings, earnigs, er's, 0qfy0, earning's, qtrly,\n",
      "    dividend:\t divi, dividends, divy, divvy, div, payout, distributions, divs,\n",
      "    subpoena:\t whistleblower, subpoenas, foia, decree, panel's, class-action, tort, whistle-blower,\n",
      "    estimate:\t projection, estimation, projected, expectation, forcast, estimated, fy, projections,\n",
      "      volume:\t volumn, vol, volumes, voume, volune, volum, volumen, voulme,\n",
      "        risk:\t reward, risks, asymmetric, risk-reward, gamble, tolerance, mitigation, tradeoff,\n",
      "          :):\t :-), ;), ;-), =), hehe, <winking_face>, (:, glty,\n",
      "    <rocket>:\t <collision>, <chart_increasing>, <airplane>, <comet>, <airplane_departure>, <volcano>, <money_with_wings>, <crescent_moon>,\n",
      "          :(:\t :-(, ugh, ;(, :/, <pensive_face>, :-/, <disappointed_face>, =(,\n",
      "      <fire>:\t <collision>, <chart_increasing>, <high_voltage>, <comet>, <heavy_dollar_sign>, <volcano>, <money_with_wings>, <bomb>,\n"
     ]
    }
   ],
   "source": [
    "evaluate(r\"F:\\w2v_model_dir_8192_shuffle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
