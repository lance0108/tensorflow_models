{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional neural networks based on:\n",
    "Cliche, M. (2017). \"BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs.\" arXiv preprint arXiv:1704.06125.\n",
    "Kim, Y. (2014). \"Convolutional neural networks for sentence classification.\" arXiv preprint arXiv:1408.5882."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lance\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from shutil import rmtree\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "seed = 123\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Neutral     5836\n",
       "Positive    2449\n",
       "Negative    1715\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(r\"C:\\Dropbox\\_projects\\word_embedding\\stocktwits\\data_final.csv\")\n",
    "df_data[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>85</td>\n",
       "      <td>4916</td>\n",
       "      <td>4917</td>\n",
       "      <td>364</td>\n",
       "      <td>85</td>\n",
       "      <td>159</td>\n",
       "      <td>1999</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>266</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>2302</td>\n",
       "      <td>3479</td>\n",
       "      <td>1478</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1289</td>\n",
       "      <td>58</td>\n",
       "      <td>29</td>\n",
       "      <td>189</td>\n",
       "      <td>5</td>\n",
       "      <td>593</td>\n",
       "      <td>6</td>\n",
       "      <td>4919</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>474</td>\n",
       "      <td>72</td>\n",
       "      <td>139</td>\n",
       "      <td>4921</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>91</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4922</td>\n",
       "      <td>1010</td>\n",
       "      <td>680</td>\n",
       "      <td>59</td>\n",
       "      <td>4923</td>\n",
       "      <td>6</td>\n",
       "      <td>3481</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4    5     6     7     8     9  ...  31  32  33  \\\n",
       "0     1   104    85  4916  4917  364    85   159  1999    65 ...   0   0   0   \n",
       "1     1    18    14   266    86    5  2302  3479  1478   150 ...   0   0   0   \n",
       "2     1    21  1289    58    29  189     5   593     6  4919 ...   0   0   0   \n",
       "3     1   474    72   139  4921    7     5    91    23     3 ...   0   0   0   \n",
       "4  4922  1010   680    59  4923    6  3481     2     4     1 ...   0   0   0   \n",
       "\n",
       "   34  35  36  37  38  39  40  \n",
       "0   0   0   0   0   0   0   0  \n",
       "1   0   0   0   0   0   0   0  \n",
       "2   0   0   0   0   0   0   0  \n",
       "3   0   0   0   0   0   0   0  \n",
       "4   0   0   0   0   0   0   0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create padded sequences\n",
    "max_len = max(map(lambda x: len(x.split(\" \")), df_data[\"tokens\"]))\n",
    "\n",
    "t = Tokenizer(split=\" \", filters=\"\")\n",
    "t.fit_on_texts(df_data[\"tokens\"])\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "sequences = t.texts_to_sequences(df_data[\"tokens\"].values)\n",
    "padded_seq = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "df_seq = pd.DataFrame(padded_seq)\n",
    "\n",
    "df_seq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the classes\n",
    "y_multi = to_categorical(df_data[\"sentiment\"].apply(lambda x: 1 if x == \"Positive\" else -1 if x == \"Negative\" else 0 ),\n",
    "              num_classes=3)\n",
    "y_single = np.argmax(y_multi, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "n_folds = 10\n",
    "train_size = int((n_folds - 1)/n_folds*len(df_data))\n",
    "steps = epochs*train_size//batch_size + 1\n",
    "def train_input_fn(x_train, y_train, batch_size, seed, epochs, buffer):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"x\":x_train}, y_train))\n",
    "    dataset = dataset.shuffle(buffer_size=buffer, seed=seed, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size).prefetch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(x_test, y_test, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\"x\":x_test}, y_test))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode, params):\n",
    "    input_layer = tf.contrib.layers.embed_sequence(\n",
    "        features[\"x\"], vocab_size, emb_dim, \n",
    "        initializer=params[\"embedding_initializer\"])\n",
    "    conv_layers = []\n",
    "    for i in range(3, 6):\n",
    "        conv_layers.append(tf.layers.conv1d(activation=tf.nn.relu,\n",
    "        inputs=input_layer, filters=100, kernel_size=i, padding=\"same\"))\n",
    "    concat = tf.concat(conv_layers, 1)\n",
    "    print(\"Concatenated shape:\", concat.shape)\n",
    "    training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "    dropout = tf.layers.dropout(inputs=concat, rate=0.2, training=training)\n",
    "    pool = tf.reduce_max(input_tensor=dropout, axis=1)\n",
    "    print(\"Pooled shape:\", pool.shape)\n",
    "    dense1 = tf.layers.dense(inputs=pool, units=30)\n",
    "    logits = tf.layers.dense(inputs=dense1, units=params[\"n_classes\"])\n",
    "    print(\"Output shape:\", logits.shape)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        preds = {\"class\":pred_indices, \"probabilities\":probs}\n",
    "        export_outputs = {\"prediction\":tf.estimator.export.PredictOutput(preds)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=preds, export_outputs=export_outputs)\n",
    "        \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        pred_indices = tf.argmax(probs, 1)\n",
    "        labels_one_hot = tf.one_hot(labels, depth=params[\"n_classes\"], on_value=True, off_value=False, dtype=tf.bool)\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels, pred_indices),\n",
    "            \"auroc\": tf.metrics.auc(labels_one_hot, probs)\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300\n",
    "model_dir = r\"D:\\word_embedding\\binary\\embeddings3\"\n",
    "column = tf.feature_column.categorical_column_with_identity('x', vocab_size)\n",
    "word_embedding_column = tf.feature_column.embedding_column(column, dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained embeddings\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\word2vec\\GoogleNews-vectors-negative300.bin',\n",
    "#                                             binary=True)\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\glove\\test_word2vec.txt',\n",
    "#                                             binary=False)\n",
    "# en_model = KeyedVectors.load_word2vec_format(r'D:\\nlp_resources\\fasttext\\wiki.en.vec',\n",
    "#                                             binary=False)\n",
    "\n",
    "en_model = KeyedVectors.load_word2vec_format(r'D:\\word_embedding\\vectors2_w2v.txt',\n",
    "                                            binary=False, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = en_model.vector_size\n",
    "embedding_matrix = np.zeros((vocab_size, emb_dim))\n",
    "for word, i in t.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i] = en_model.get_vector(word)\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_initializer(shape=None, dtype=tf.float32, partition_info=None):\n",
    "    assert dtype is tf.float32\n",
    "    return embedding_matrix\n",
    "\n",
    "pretrained_embedding_column = tf.feature_column.embedding_column(column, \n",
    "                                                                dimension=300,\n",
    "                                                                initializer=my_initializer,\n",
    "                                                                trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D5D423828>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3091316, step = 0\n",
      "INFO:tensorflow:global_step/sec: 131.104\n",
      "INFO:tensorflow:loss = 0.6781888, step = 100 (0.764 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.901\n",
      "INFO:tensorflow:loss = 0.41567826, step = 200 (0.524 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.59647137.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "1\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D050CE7B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.94973385, step = 0\n",
      "INFO:tensorflow:global_step/sec: 127.063\n",
      "INFO:tensorflow:loss = 0.60541326, step = 100 (0.789 sec)\n",
      "INFO:tensorflow:global_step/sec: 178.872\n",
      "INFO:tensorflow:loss = 0.40698093, step = 200 (0.559 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.54305124.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "2\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D0F624CC0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.3926241, step = 0\n",
      "INFO:tensorflow:global_step/sec: 129.872\n",
      "INFO:tensorflow:loss = 0.5531734, step = 100 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.537\n",
      "INFO:tensorflow:loss = 0.3940932, step = 200 (0.524 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.6146654.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "3\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D621C7400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1143239, step = 0\n",
      "INFO:tensorflow:global_step/sec: 128.082\n",
      "INFO:tensorflow:loss = 0.6558368, step = 100 (0.782 sec)\n",
      "INFO:tensorflow:global_step/sec: 196.437\n",
      "INFO:tensorflow:loss = 0.40718997, step = 200 (0.509 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5741859.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "4\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D0E6772E8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3415303, step = 0\n",
      "INFO:tensorflow:global_step/sec: 123.006\n",
      "INFO:tensorflow:loss = 0.64330924, step = 100 (0.814 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.901\n",
      "INFO:tensorflow:loss = 0.42450264, step = 200 (0.523 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.4244596.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "5\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D135481D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.555228, step = 0\n",
      "INFO:tensorflow:global_step/sec: 136.425\n",
      "INFO:tensorflow:loss = 0.91137224, step = 100 (0.734 sec)\n",
      "INFO:tensorflow:global_step/sec: 190.175\n",
      "INFO:tensorflow:loss = 0.51500034, step = 200 (0.525 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.43219936.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "6\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D61FF21D0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.0007459, step = 0\n",
      "INFO:tensorflow:global_step/sec: 128.374\n",
      "INFO:tensorflow:loss = 0.68018675, step = 100 (0.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 188.03\n",
      "INFO:tensorflow:loss = 0.40181783, step = 200 (0.533 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.3491882.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "7\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D61F33898>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1797533, step = 0\n",
      "INFO:tensorflow:global_step/sec: 129.743\n",
      "INFO:tensorflow:loss = 0.69933647, step = 100 (0.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 194.93\n",
      "INFO:tensorflow:loss = 0.37873572, step = 200 (0.513 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.30846745.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "8\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D0641F7F0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2996188, step = 0\n",
      "INFO:tensorflow:global_step/sec: 135.315\n",
      "INFO:tensorflow:loss = 0.6245337, step = 100 (0.741 sec)\n",
      "INFO:tensorflow:global_step/sec: 193.86\n",
      "INFO:tensorflow:loss = 0.3406333, step = 200 (0.516 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23840946.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "9\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'D:\\\\word_embedding\\\\binary\\\\embeddings3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x0000022D65C116D8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:loss = 1.9597402, step = 0\n",
      "INFO:tensorflow:global_step/sec: 142.293\n",
      "INFO:tensorflow:loss = 0.63866884, step = 100 (0.705 sec)\n",
      "INFO:tensorflow:global_step/sec: 218.77\n",
      "INFO:tensorflow:loss = 0.48900023, step = 200 (0.456 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 282 into D:\\word_embedding\\binary\\embeddings3\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.5042789.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "Concatenated shape: (?, 123, 100)\n",
      "Pooled shape: (?, 100)\n",
      "Output shape: (?, 3)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from D:\\word_embedding\\binary\\embeddings3\\model.ckpt-282\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "0.6722 0.6826\n"
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "accuracy = []\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed)\n",
    "fold = 0\n",
    "for train_idx, test_idx in skf.split(padded_seq, y_single):\n",
    "    print(fold)\n",
    "    fold+=1\n",
    "    x_train = padded_seq[train_idx]\n",
    "    x_test = padded_seq[test_idx]\n",
    "    y_train = y_single[train_idx]\n",
    "    y_test = y_single[test_idx]\n",
    "    \n",
    "    rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "    params = {'embedding_initializer': my_initializer,\n",
    "#         'embedding_initializer': tf.random_uniform_initializer(-1.0, 1.0),\n",
    "              'n_classes':3}\n",
    "    classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                       model_dir=model_dir,\n",
    "                                       params=params)\n",
    "    classifier.train(input_fn=lambda :train_input_fn(x_train, y_train, batch_size, seed, epochs, train_size), steps=steps)\n",
    "    pred =  classifier.predict(input_fn=lambda : eval_input_fn(x_test, y_test, batch_size))\n",
    "    y_true = y_single[test_idx]\n",
    "    y_pred = [p[\"class\"] for p in pred]\n",
    "    f1.append(f1_score(y_true, y_pred, average=\"weighted\"))\n",
    "    accuracy.append(accuracy_score(y_true, y_pred))\n",
    "print(round(np.mean(f1), 4), round(np.mean(accuracy), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
