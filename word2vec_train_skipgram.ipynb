{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Train skip grams word embeddings using multiple GPUs.</strong><br>\n",
    "Hyperparameter settings based on:<br>\n",
    "Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "emb_dim = 300\n",
    "vocab_size = 539952"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = r\"D:\\stocktwits_text\\2018 all\\skipgrams_noneg\"\n",
    "file_names = os.listdir(in_dir)\n",
    "file_names = [os.path.join(in_dir, file_name) for file_name in file_names]\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_counts = [586828054, 677051750, 598760681, 583052985, 547069608,\n",
    "               536936140, 551959422, 552639908, 551994643, 571124852]\n",
    "steps = math.ceil(sum(line_counts)/batch_size)\n",
    "steps_gpu = math.ceil(steps/2)\n",
    "# each 100 step takes about 12 seconds\n",
    "minutes = int(steps_gpu/100*12/60)\n",
    "print(\"Total steps:\", steps)\n",
    "print(\"Total steps with 2 GPUs:\", steps_gpu)\n",
    "print(\"Total estimated minutes (GPU):\", minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_line(line):\n",
    "    fields = tf.decode_csv(line, [[0], [0]])\n",
    "    return {\"word\": fields[0]}, fields[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn_distributed():\n",
    "    dataset = tf.data.TextLineDataset(file_names)\n",
    "    dataset = dataset.shuffle(batch_size*100)\n",
    "    dataset = dataset.batch(batch_size).map(_parse_line, num_parallel_calls=2).prefetch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    dataset = tf.data.TextLineDataset(file_names)\n",
    "    dataset = dataset.shuffle(batch_size*100)\n",
    "    dataset = dataset.batch(batch_size).map(_parse_line, num_parallel_calls=2).prefetch(batch_size)\n",
    "    data_iter = dataset.make_one_shot_iterator()\n",
    "    return data_iter.get_next()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_model_fn(features, labels, mode):\n",
    "    with tf.name_scope(\"embeddings\"):\n",
    "        embeddings = tf.get_variable(\"embedding\", shape=[vocab_size, emb_dim])\n",
    "        embed = tf.nn.embedding_lookup(embeddings, features[\"word\"])\n",
    "        print(\"Embedded shape:\", embed.shape)\n",
    "    with tf.name_scope(\"weights\"):\n",
    "        nce_weights = tf.get_variable(\"W\", shape=[vocab_size, emb_dim])\n",
    "        print(\"nce_weights shape:\", nce_weights.shape)\n",
    "    with tf.name_scope(\"biases\"):\n",
    "        nce_biases = tf.get_variable(\"b\", shape=[vocab_size])\n",
    "        print(\"nce_biases shape:\", nce_biases.shape)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "            weights=nce_weights, biases=nce_biases,\n",
    "            inputs=embed, labels=labels[:, None], \n",
    "            num_sampled=5, num_classes=vocab_size))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.scalar(\"my_global_step\", tf.train.get_global_step())\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # batch 65536; no device specification\n",
    "# classifier = tf.estimator.Estimator(model_fn=sg_model_fn, model_dir=r\"F:\\tf_model_dir6\")\n",
    "# classifier.train(input_fn=train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=2, prefetch_on_device=True)\n",
    "config = tf.estimator.RunConfig(\n",
    "    save_summary_steps=500,\n",
    "    train_distribute=strategy,\n",
    "#     save_checkpoints_secs = 20*60,\n",
    "    save_checkpoints_steps = 5000,\n",
    "    keep_checkpoint_max = 3,\n",
    "    model_dir=r\"F:\\w2v_model_dir_4096_shuffle\")\n",
    "classifier = tf.estimator.Estimator(model_fn=sg_model_fn, \n",
    "                                    config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifier.train(input_fn=train_input_fn_distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
